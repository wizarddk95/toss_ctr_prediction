{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa36f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290e951c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model with memory-efficient iterator...\n",
      "Streaming from ./data/train_optimized.parquet...\n",
      "Streaming from ./data/train_optimized.parquet...\n",
      "Streaming from ./data/train_optimized.parquet...\n",
      "Streaming from ./data/train_optimized.parquet...\n",
      "Streaming from ./data/train_optimized.parquet...\n",
      "Streaming from ./data/train_optimized.parquet...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "class CorpusIterator:\n",
    "    \"\"\"\n",
    "    Parquet 파일을 배치 단위로 읽어 메모리 효율적으로 말뭉치를 생성하는 이터레이터.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_paths, batch_size=100_000, column='seq'):\n",
    "        self.file_paths = file_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.column = column\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_path in self.file_paths:\n",
    "            print(f\"Streaming from {file_path}...\")\n",
    "            # Parquet 파일을 열고 배치 단위로 순회\n",
    "            parquet_file = pq.ParquetFile(file_path)\n",
    "            for batch in parquet_file.iter_batches(batch_size=self.batch_size, columns=[self.column]):\n",
    "                # Arrow Batch를 Pandas Series로 변환\n",
    "                series = batch.to_pandas()[self.column]\n",
    "                # 각 행의 문자열을 쉼표로 분리\n",
    "                for sentence in series.str.split(','):\n",
    "                    # 비어있는 요소 '' 제거 후 문장(리스트)을 반환(yield)\n",
    "                    # sentence가 None일 경우를 대비하여 빈 리스트로 처리\n",
    "                    yield [word for word in (sentence or []) if word]\n",
    "\n",
    "# --- 최종 실행 ---\n",
    "# 처리할 파일 경로 리스트\n",
    "# file_paths = ['./data/train_optimized.parquet', './data/test_optimized.parquet']\n",
    "file_paths = ['./data/train_optimized.parquet']\n",
    "\n",
    "# 이터레이터 객체 생성\n",
    "# 이 시점에는 아직 데이터를 읽지 않습니다.\n",
    "corpus_iterator = CorpusIterator(file_paths)\n",
    "\n",
    "# Word2Vec 모델 학습 시, corpus_iterator를 sentences 인자로 직접 전달\n",
    "# 모델이 필요할 때마다 데이터를 조금씩 스트리밍하여 사용합니다.\n",
    "VECTOR_SIZE = 64\n",
    "WINDOW = 5\n",
    "MIN_COUNT = 3\n",
    "WORKERS = multiprocessing.cpu_count()\n",
    "SG = 1\n",
    "\n",
    "print(\"Training Word2Vec model with memory-efficient iterator...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=corpus_iterator,  # <--- 메모리에 모든 데이터를 올리는 대신 이터레이터 사용!\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    workers=WORKERS,\n",
    "    sg=SG,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "w2v_model.save(\"./models/item2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a23ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence, model, vector_size):\n",
    "    \"\"\"문장(ID 리스트)을 벡터화 (numpy 연산)\"\"\"\n",
    "    words = [w for w in sentence if w in model.wv]\n",
    "    if not words:\n",
    "        return np.zeros(vector_size, dtype=np.float32)\n",
    "    arr = model.wv[words]  # (len(words), vector_size) numpy 배열\n",
    "    return arr.mean(axis=0)\n",
    "\n",
    "def build_seq_vectors(df, model, vector_size, col=\"seq\", batch_size=100_000):\n",
    "    \"\"\"대규모 데이터를 배치 단위로 변환\"\"\"\n",
    "    vec_list = []\n",
    "    total = len(df)\n",
    "    \n",
    "    for start in tqdm(range(0, total, batch_size)):\n",
    "        end = min(start + batch_size, total)\n",
    "        batch = df[col].iloc[start:end].str.split(\",\")\n",
    "        batch_vecs = np.vstack([\n",
    "            get_sentence_vector(seq, model, vector_size) for seq in batch\n",
    "        ])\n",
    "        vec_list.append(batch_vecs)\n",
    "\n",
    "    all_vecs = np.vstack(vec_list)\n",
    "\n",
    "    return all_vecs\n",
    "    \n",
    "\n",
    "train_seq = pd.read_parquet('./data/train_optimized.parquet', columns=['seq'])\n",
    "test_seq = pd.read_parquet('./data/test_optimized.parquet', columns=['seq'])\n",
    "\n",
    "# 실행\n",
    "print(\"Creating sequence vectors for train and test data...\")\n",
    "\n",
    "train_seq_vectors = build_seq_vectors(\n",
    "    train_seq, w2v_model, VECTOR_SIZE, col=\"seq\",\n",
    ")\n",
    "\n",
    "test_seq_vectors = build_seq_vectors(\n",
    "    test_seq, w2v_model, VECTOR_SIZE, col=\"seq\",\n",
    ")\n",
    "\n",
    "print(\"Sequence vectors created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a24e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전에 만들어둔 베이스라인 데이터셋을 불러옵니다.\n",
    "# (예: train_baseline.parquet)\n",
    "train_baseline_df = pd.read_parquet('./data/train_optimized.parquet') \n",
    "test_baseline_df = pd.read_parquet('./data/test_optimized.parquet')\n",
    "\n",
    "train_seq_stats = pd.read_parquet('./data/processed/train_seq_stats.parquet')\n",
    "test_seq_stats = pd.read_parquet('./data/processed/test_seq_stats.parquet')\n",
    "\n",
    "train_seq_vec_df = pd.DataFrame(train_seq_vectors, columns=[f\"seq_vec_{i}\" for i in range(VECTOR_SIZE)])\n",
    "test_seq_vec_df = pd.DataFrame(test_seq_vectors, columns=[f\"seq_vec_{i}\" for i in range(VECTOR_SIZE)])\n",
    "\n",
    "seq_cols = ['seq_len', 'seq_first', 'seq_last']\n",
    "\n",
    "# 벡터 피처들을 옆으로 이어 붙입니다.\n",
    "train_final = pd.concat([train_baseline_df, train_seq_stats[seq_cols], train_seq_vec_df], axis=1)\n",
    "test_final = pd.concat([test_baseline_df, test_seq_stats[seq_cols], test_seq_vec_df], axis=1)\n",
    "\n",
    "print(\"Final dataset is ready.\")\n",
    "print(\"New train shape:\", train_final.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
