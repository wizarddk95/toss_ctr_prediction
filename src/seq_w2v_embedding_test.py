"""
í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ Word2Vec ì„ë² ë”© ì•™ìƒë¸” ìƒì„±
- 5ê°œ í´ë“œ ëª¨ë¸ë¡œ ê°ê° ì„ë² ë”© ìƒì„±
- í´ë“œë³„ ì„ë² ë”© ì €ì¥
- ìµœì¢… ì•™ìƒë¸”(í‰ê· ) ìƒì„±
"""

import os
import gc
import time
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from typing import List, Dict, Optional
from gensim.models import Word2Vec


# =========================
# ì„¤ì •
# =========================
# íŒŒì¼ ê²½ë¡œ
TEST_COMPRESSED_PATH = './data/seq_compression/test_seq_compressed.parquet'
MODEL_DIR = './models/w2v'
OUTPUT_DIR = './data/seq_w2v_embedding/test'

# í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ì„¤ì •
SEQ_COL = 'seq_compressed'
BATCH_SIZE = 200_000
VECTOR_SIZE = 64
LAST_K_LIST = (5, 20, 50)
N_SPLITS = 5

# ì •ë°€ë„ ì„¤ì • (í•™ìŠµ ì‹œì™€ ë™ì¼)
FLOAT_PRECISION = 4
USE_FLOAT16 = False

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)


# =========================
# ìœ í‹¸ í•¨ìˆ˜
# =========================
def reduce_precision(arr: np.ndarray, precision: int = 4, use_float16: bool = False) -> np.ndarray:
    """ì •ë°€ë„ ê°ì†Œ"""
    arr_rounded = np.round(arr, precision)
    return arr_rounded.astype(np.float16) if use_float16 else arr_rounded.astype(np.float32)


def process_tokens_to_embeddings(tokens: List[str], 
                                w2v: Word2Vec, 
                                last_k_list: tuple,
                                precision: int = 4,
                                use_float16: bool = False) -> Dict:
    """í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
    vecs = []
    oov_count = 0
    
    # í† í°ì„ ë²¡í„°ë¡œ ë³€í™˜
    for token in tokens:
        if token in w2v.wv:
            vecs.append(w2v.wv[token])
        else:
            oov_count += 1
    
    d = w2v.vector_size
    
    if not vecs:
        # ëª¨ë“  í† í°ì´ OOVì¸ ê²½ìš°
        zero = np.zeros(d, dtype=np.float32)
        reps = {'mean': zero, 'last': zero, 'max': zero}
        for k in last_k_list:
            reps[f'last{k}'] = zero
    else:
        V = np.vstack(vecs).astype(np.float32)
        
        # ë‹¤ì–‘í•œ ëŒ€í‘œ ì„ë² ë”© ê³„ì‚°
        reps = {
            'mean': V.mean(axis=0),
            'last': V[-1],
            'max': V.max(axis=0)
        }
        
        # Last-K ì„ë² ë”©
        for k in last_k_list:
            if len(V) >= k:
                reps[f'last{k}'] = V[-k:].mean(axis=0)
            else:
                reps[f'last{k}'] = V.mean(axis=0)
    
    # ì •ë°€ë„ ê°ì†Œ
    for name in reps:
        reps[name] = reduce_precision(reps[name], precision, use_float16)
    
    # ë”•ì…”ë„ˆë¦¬ë¡œ í¼ì¹˜ê¸°
    rec = {}
    for name, vec in reps.items():
        for i in range(vec.shape[0]):
            rec[f'{name}_{i}'] = float(vec[i])
    
    # OOV ë¹„ìœ¨ ì¶”ê°€
    rec['oov_ratio'] = oov_count / len(tokens) if tokens else 0.0
    
    return rec


def generate_test_embeddings_single_fold(
    test_path: str,
    model_path: str,
    output_path: str,
    fold: int,
    batch_size: int = 200_000
) -> bool:
    """
    ë‹¨ì¼ í´ë“œ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    print(f"\n--- Fold {fold} ---")
    
    # ëª¨ë¸ ë¡œë“œ
    if not os.path.exists(model_path):
        print(f"  âŒ Model not found: {model_path}")
        return False
    
    print(f"  Loading model: {model_path}")
    w2v_model = Word2Vec.load(model_path)
    vocab_size = len(w2v_model.wv)
    print(f"  Vocabulary size: {vocab_size:,}")
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
    print(f"  Processing test file...")
    writer = None
    row_id = 0
    total_oov = 0
    total_tokens = 0
    
    try:
        pf = pq.ParquetFile(test_path)
        total_rows = pf.metadata.num_rows
        print(f"  Total test rows: {total_rows:,}")
        
        for batch_idx, batch in enumerate(pf.iter_batches(batch_size=batch_size, columns=[SEQ_COL])):
            df_batch = batch.to_pandas()
            
            # í† í°í™”
            tokens_series = df_batch[SEQ_COL].fillna('').str.split(',')
            
            # ê° í† í° ë¦¬ìŠ¤íŠ¸ ì •ì œ
            tokens_series = tokens_series.apply(
                lambda toks: [t.strip() for t in toks if t.strip()]
            )
            
            # ì„ë² ë”© ìƒì„±
            embedding_records = tokens_series.apply(
                lambda tokens: process_tokens_to_embeddings(
                    tokens, w2v_model, LAST_K_LIST, FLOAT_PRECISION, USE_FLOAT16
                )
            )
            
            # DataFrame ìƒì„±
            df = pd.DataFrame.from_records(embedding_records.tolist())
            df['row_id'] = range(row_id, row_id + len(df))
            row_id += len(df)
            
            # OOV í†µê³„ ìˆ˜ì§‘
            total_oov += df['oov_ratio'].sum() * len(df)
            total_tokens += len(df)
            
            # float16 ë³€í™˜
            if USE_FLOAT16:
                float_cols = [col for col in df.columns if col not in ['row_id', 'oov_ratio']]
                for col in float_cols:
                    df[col] = df[col].astype(np.float16)
            
            # íŒŒì¼ ì €ì¥
            table = pa.Table.from_pandas(df, preserve_index=False)
            if writer is None:
                writer = pq.ParquetWriter(output_path, table.schema, compression='snappy')
            writer.write_table(table)
            
            # ì§„í–‰ ìƒí™©
            if batch_idx % 5 == 0:
                progress = (row_id / total_rows) * 100
                print(f"    Progress: {progress:.1f}% ({row_id:,}/{total_rows:,})", end='\r')
        
        print()  # ì¤„ë°”ê¿ˆ
        
    finally:
        if writer:
            writer.close()
    
    # í†µê³„ ì¶œë ¥
    avg_oov = (total_oov / total_tokens) * 100 if total_tokens > 0 else 0
    print(f"  âœ… Saved: {output_path}")
    print(f"  Average OOV rate: {avg_oov:.2f}%")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del w2v_model
    gc.collect()
    
    return True


def create_ensemble_average(fold_paths: List[str], output_path: str):
    """
    ì—¬ëŸ¬ í´ë“œì˜ ì„ë² ë”©ì„ í‰ê· ë‚´ì–´ ì•™ìƒë¸” ìƒì„±
    """
    print("\n" + "="*70)
    print("Creating Ensemble (Average)")
    print("="*70)
    
    # ìœ íš¨í•œ í´ë“œ íŒŒì¼ë§Œ í•„í„°ë§
    valid_paths = [p for p in fold_paths if os.path.exists(p)]
    
    if len(valid_paths) == 0:
        print("âŒ No fold embeddings found!")
        return
    
    if len(valid_paths) < len(fold_paths):
        print(f"âš ï¸ Warning: Only {len(valid_paths)}/{len(fold_paths)} fold files found")
    
    print(f"Averaging {len(valid_paths)} fold embeddings...")
    
    # ì²« ë²ˆì§¸ íŒŒì¼ë¡œ êµ¬ì¡° í™•ì¸
    first_df = pd.read_parquet(valid_paths[0])
    n_rows = len(first_df)
    print(f"  Rows per fold: {n_rows:,}")
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
    batch_size = 50000
    n_batches = (n_rows + batch_size - 1) // batch_size
    
    writer = None
    
    for batch_idx in range(n_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, n_rows)
        
        print(f"  Processing batch {batch_idx+1}/{n_batches} (rows {start_idx:,}-{end_idx:,})...", end='\r')
        
        # ê° í´ë“œì—ì„œ ë°°ì¹˜ ì½ê¸°
        batch_dfs = []
        for path in valid_paths:
            df = pd.read_parquet(path).iloc[start_idx:end_idx]
            df = df.sort_values('row_id').reset_index(drop=True)
            batch_dfs.append(df)
        
        # row_idì™€ ê¸°íƒ€ ë©”íƒ€ ì •ë³´ëŠ” ì²« ë²ˆì§¸ í´ë“œì—ì„œ ê°€ì ¸ì˜´
        ensemble_df = batch_dfs[0][['row_id']].copy()
        
        # OOV ratioëŠ” í‰ê· 
        if 'oov_ratio' in batch_dfs[0].columns:
            oov_ratios = np.stack([df['oov_ratio'].values for df in batch_dfs])
            ensemble_df['oov_ratio'] = oov_ratios.mean(axis=0)
        
        # ì„ë² ë”© ì»¬ëŸ¼ë“¤ í‰ê· 
        embedding_cols = [col for col in batch_dfs[0].columns 
                         if col not in ['row_id', 'oov_ratio']]
        
        for col in embedding_cols:
            col_values = np.stack([df[col].values for df in batch_dfs])
            mean_values = col_values.mean(axis=0)
            
            # ì •ë°€ë„ ê°ì†Œ
            if FLOAT_PRECISION:
                mean_values = np.round(mean_values, FLOAT_PRECISION)
            if USE_FLOAT16:
                mean_values = mean_values.astype(np.float16)
            
            ensemble_df[col] = mean_values
        
        # íŒŒì¼ ì €ì¥
        table = pa.Table.from_pandas(ensemble_df, preserve_index=False)
        if writer is None:
            writer = pq.ParquetWriter(output_path, table.schema, compression='snappy')
        writer.write_table(table)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del batch_dfs, ensemble_df
        gc.collect()
    
    print()  # ì¤„ë°”ê¿ˆ
    
    if writer:
        writer.close()
    
    print(f"âœ… Ensemble saved: {output_path}")
    
    # íŒŒì¼ í¬ê¸° ì •ë³´
    for path in valid_paths + [output_path]:
        if os.path.exists(path):
            size_mb = os.path.getsize(path) / (1024 * 1024)
            print(f"  - {os.path.basename(path)}: {size_mb:.1f} MB")


def verify_embeddings(fold_paths: List[str], ensemble_path: str, sample_size: int = 5):
    """
    ìƒì„±ëœ ì„ë² ë”© ê²€ì¦
    """
    print("\n" + "="*70)
    print("Verification")
    print("="*70)
    
    # ì•™ìƒë¸” íŒŒì¼ í™•ì¸
    if os.path.exists(ensemble_path):
        ensemble_df = pd.read_parquet(ensemble_path)
        print(f"\nEnsemble shape: {ensemble_df.shape}")
        print(f"Columns: {ensemble_df.shape[1]} features")
        print(f"Memory usage: {ensemble_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        # ìƒ˜í”Œ í†µê³„
        print(f"\nSample statistics (first {sample_size} rows):")
        sample = ensemble_df.head(sample_size)
        
        # ì„ë² ë”© ì»¬ëŸ¼ë“¤ì˜ í†µê³„
        embedding_cols = [col for col in ensemble_df.columns 
                         if col not in ['row_id', 'oov_ratio']]
        
        if embedding_cols:
            print(f"  Mean embedding values:")
            for i in range(min(3, len(embedding_cols))):
                col = embedding_cols[i]
                print(f"    {col}: {ensemble_df[col].mean():.4f} (Â±{ensemble_df[col].std():.4f})")
        
        if 'oov_ratio' in ensemble_df.columns:
            print(f"\n  OOV statistics:")
            print(f"    Mean: {ensemble_df['oov_ratio'].mean():.3f}")
            print(f"    Max:  {ensemble_df['oov_ratio'].max():.3f}")
            print(f"    Min:  {ensemble_df['oov_ratio'].min():.3f}")
    
    # í´ë“œ ê°„ ì¼ê´€ì„± í™•ì¸
    print(f"\nFold consistency check:")
    valid_paths = [p for p in fold_paths if os.path.exists(p)]
    
    if len(valid_paths) >= 2:
        # ì²« ë²ˆì§¸ í–‰ì˜ ëª‡ ê°œ ê°’ ë¹„êµ
        row_0_values = []
        for path in valid_paths[:2]:
            df = pd.read_parquet(path, columns=['mean_0', 'mean_1', 'mean_2']).head(1)
            values = df[['mean_0', 'mean_1', 'mean_2']].values[0]
            row_0_values.append(values)
            print(f"  {os.path.basename(path)}: {values}")
        
        # ì°¨ì´ ê³„ì‚°
        if len(row_0_values) == 2:
            diff = np.abs(row_0_values[0] - row_0_values[1])
            print(f"  Absolute difference: {diff}")


# =========================
# ë©”ì¸ ì‹¤í–‰
# =========================
def main():
    print("="*70)
    print("TEST SET WORD2VEC EMBEDDING GENERATION")
    print("="*70)
    
    # ì„¤ì • ì¶œë ¥
    print(f"\nConfiguration:")
    print(f"  Test file: {TEST_COMPRESSED_PATH}")
    print(f"  Model directory: {MODEL_DIR}")
    print(f"  Output directory: {OUTPUT_DIR}")
    print(f"  Number of folds: {N_SPLITS}")
    print(f"  Precision: {FLOAT_PRECISION} decimals")
    print(f"  Data type: {'float16' if USE_FLOAT16 else 'float32'}")
    
    # íŒŒì¼ í™•ì¸
    if not os.path.exists(TEST_COMPRESSED_PATH):
        print(f"\nâŒ Error: Test file not found: {TEST_COMPRESSED_PATH}")
        return
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë³´
    pf = pq.ParquetFile(TEST_COMPRESSED_PATH)
    test_rows = pf.metadata.num_rows
    print(f"\nTest data: {test_rows:,} rows")
    
    print("\n" + "="*70)
    print("STEP 1: Generate embeddings for each fold")
    print("="*70)
    
    # ê° í´ë“œë³„ë¡œ ì„ë² ë”© ìƒì„±
    fold_paths = []
    successful_folds = 0
    
    for fold in range(N_SPLITS):
        model_path = os.path.join(MODEL_DIR, f'w2v_fold{fold}.model')
        output_path = os.path.join(OUTPUT_DIR, f'test_fold{fold}.parquet')
        
        success = generate_test_embeddings_single_fold(
            test_path=TEST_COMPRESSED_PATH,
            model_path=model_path,
            output_path=output_path,
            fold=fold,
            batch_size=BATCH_SIZE
        )
        
        if success:
            fold_paths.append(output_path)
            successful_folds += 1
        else:
            print(f"  âš ï¸ Skipping fold {fold}")
    
    print(f"\nâœ… Generated embeddings for {successful_folds}/{N_SPLITS} folds")
    
    if successful_folds == 0:
        print("âŒ No embeddings generated. Exiting.")
        return
    
    # ì•™ìƒë¸” ìƒì„±
    print("\n" + "="*70)
    print("STEP 2: Create ensemble (average)")
    print("="*70)
    
    ensemble_path = os.path.join(OUTPUT_DIR, 'test_ensemble.parquet')
    create_ensemble_average(fold_paths, ensemble_path)
    
    # ê²€ì¦
    verify_embeddings(fold_paths, ensemble_path)
    
    # ì™„ë£Œ ë©”ì‹œì§€
    print("\n" + "="*70)
    print("âœ… COMPLETED SUCCESSFULLY!")
    print("="*70)
    
    print(f"\nGenerated files:")
    print(f"  Individual folds:")
    for fold in range(N_SPLITS):
        path = os.path.join(OUTPUT_DIR, f'test_fold{fold}.parquet')
        if os.path.exists(path):
            size_mb = os.path.getsize(path) / (1024 * 1024)
            print(f"    - test_fold{fold}.parquet ({size_mb:.1f} MB)")
    
    print(f"\n  Ensemble:")
    if os.path.exists(ensemble_path):
        size_mb = os.path.getsize(ensemble_path) / (1024 * 1024)
        print(f"    - test_ensemble.parquet ({size_mb:.1f} MB)")
    
    # ì‚¬ìš© ë°©ë²•
    print(f"\nğŸ“ Usage example:")
    print(f"```python")
    print(f"import pandas as pd")
    print(f"")
    print(f"# Load ensemble embeddings")
    print(f"test_embeddings = pd.read_parquet('{ensemble_path}')")
    print(f"")
    print(f"# Or load specific fold")
    print(f"fold0_embeddings = pd.read_parquet('{OUTPUT_DIR}/test_fold0.parquet')")
    print(f"```")


if __name__ == "__main__":
    start_time = time.time()
    main()
    elapsed = time.time() - start_time
    print(f"\nâ±ï¸ Total execution time: {elapsed:.1f} seconds")